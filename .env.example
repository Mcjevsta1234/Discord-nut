# ========================================
# DISCORD-NUT ENVIRONMENT CONFIGURATION
# ========================================
# Copy this file to .env and fill in your credentials.
# NEVER commit .env to git - it contains secrets.

# ========================================
# Discord Configuration (Required)
# ========================================

# Bot token from Discord Developer Portal
# https://discord.com/developers/applications
DISCORD_TOKEN=your_discord_bot_token_here

# Client ID from Discord Developer Portal
DISCORD_CLIENT_ID=your_discord_client_id_here

# ========================================
# OpenRouter Configuration (Required)
# ========================================

# API key from https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# OpenRouter API base URL (optional, defaults to production)
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ========================================
# Console Mode Configuration (Optional)
# ========================================

# Run in console-only mode (no Discord connection)
# CONSOLE_MODE=true

# Run in hybrid mode (Discord + Console simultaneously)
# HYBRID_MODE=true

# Note: If DISCORD_TOKEN is omitted, bot automatically uses console-only mode

# ========================================
# Environment & Debugging (Optional)
# ========================================

# Node environment (affects logging verbosity)
# NODE_ENV=development

# ========================================
# Prompt Caching (Optional)
# ========================================

# Enable prompt caching for cost savings (default: enabled)
# Caching reduces costs for repeated queries by ~50%
# Set to '0' to disable
OPENROUTER_PROMPT_CACHE=1

# ========================================
# Model Routing Configuration (Optional)
# ========================================

# Routing strategy: 'heuristic' (fast), 'routerModel' (intelligent), or 'hybrid'
ROUTING_MODE=hybrid

# Primary router model for intelligent routing decisions (fast, cheap)
# Used when ROUTING_MODE is 'routerModel' or 'hybrid'
MODEL_ROUTER=mistralai/mistral-7b-instruct:free

# Fallback router model if primary fails (rate limits, errors)
MODEL_ROUTER_FALLBACK=meta-llama/llama-3.2-3b-instruct:free

# Confidence threshold (0-100): if heuristic confidence below this, use router model
ROUTING_CONFIDENCE_THRESHOLD=80

# Retry configuration
ROUTING_MAX_RETRIES=1
ROUTING_RETRY_HIGHER_TIER=true
ROUTING_BACKOFF_MS=1000

# ========================================
# Model Tier Configurations (Optional)
# ========================================
# Each tier serves a specific purpose. Override defaults as needed.

# INSTANT TIER: Fast, cheap - greetings, simple queries, small talk
MODEL_INSTANT=qwen/qwen3-4b:free
MODEL_INSTANT_MAX_PROMPT=8192
MODEL_INSTANT_MAX_OUTPUT=2048
MODEL_INSTANT_INPUT_PRICE=0
MODEL_INSTANT_OUTPUT_PRICE=0

# SMART TIER: General-purpose - most conversations, tool usage (default)
MODEL_SMART=deepseek/deepseek-r1-0528:free
MODEL_SMART_MAX_PROMPT=131072
MODEL_SMART_MAX_OUTPUT=32000
MODEL_SMART_INPUT_PRICE=0
MODEL_SMART_OUTPUT_PRICE=0

# THINKING TIER: Deep reasoning - complex analysis, detailed explanations
MODEL_THINKING=allenai/olmo-3.1-32b-think:free
MODEL_THINKING_MAX_PROMPT=64000
MODEL_THINKING_MAX_OUTPUT=16000
MODEL_THINKING_INPUT_PRICE=0
MODEL_THINKING_OUTPUT_PRICE=0

# CODING TIER: Code-specialized - generation, debugging, refactoring
# NOTE: Code models REQUIRE prompt caching to work efficiently
# Caching enables context reuse and significant cost savings
MODEL_CODING=google/gemini-3-flash-preview
MODEL_CODING_MAX_PROMPT=128000
MODEL_CODING_MAX_OUTPUT=128000
MODEL_CODING_INPUT_PRICE=0.50
MODEL_CODING_OUTPUT_PRICE=3.00
MODEL_CODING_CACHE_READ_PRICE=0.05

# ========================================
# Pricing Configuration (Optional)
# ========================================
# Prices in USD per 1 million tokens. Set these to track costs in debug output.
# The system maintains a fallback pricing table in src/ai/llmMetadata.ts,
# but you can override per-tier pricing here.
#
# Three-part pricing structure:
#   INPUT_PRICE: Cost per 1M input tokens
#   OUTPUT_PRICE: Cost per 1M output tokens
#   CACHE_READ_PRICE: Cost per 1M cached tokens (optional, for models with cache support)
#
# Examples:
#   - GPT-4o: INPUT=2.50, OUTPUT=10.00
#   - Claude 3.5 Sonnet: INPUT=3.00, OUTPUT=15.00
#   - Gemini 3 Flash: INPUT=0.50, OUTPUT=3.00, CACHE_READ=0.05
#   - Gemini 1.5 Flash: INPUT=0.075, OUTPUT=0.30, CACHE_READ=0.0075
#   - Free models: INPUT=0, OUTPUT=0, CACHE_READ=0
#
# Current coding tier (Gemini 3 Flash) with cache support:
MODEL_CODING_INPUT_PRICE=0.50
MODEL_CODING_OUTPUT_PRICE=3.00
MODEL_CODING_CACHE_READ_PRICE=0.05
#
# Free tier overrides (uncomment to enable)
# MODEL_INSTANT_INPUT_PRICE=0
# MODEL_INSTANT_OUTPUT_PRICE=0

# Smart tier overrides (uncomment to enable)
# MODEL_SMART_INPUT_PRICE=0
# MODEL_SMART_OUTPUT_PRICE=0

# Thinking tier overrides (uncomment to enable)
# MODEL_THINKING_INPUT_PRICE=0
# MODEL_THINKING_OUTPUT_PRICE=0

# ========================================
# Image Generation Configuration (Optional)
# ========================================

# Image generation model from OpenRouter
# Recommended: google/gemini-2.5-flash-image (free, reliable)
IMAGE_MODEL=google/gemini-2.5-flash-image

# Default image resolution (width x height)
IMAGE_DEFAULT_WIDTH=512
IMAGE_DEFAULT_HEIGHT=512

# Maximum allowed resolution (prevents excessive costs/sizes)
IMAGE_MAX_WIDTH=1024
IMAGE_MAX_HEIGHT=1024

# Supported resolutions:
#   Square: 128×128, 256×256, 512×512, 768×768, 1024×1024
#   16:9 landscape: 768×432, 1024×576
#   9:16 portrait: 432×768, 576×1024
#   Banner: 1024×256
# Note: Discord has 8MB file size limit; images are auto-compressed if needed

# ========================================
# Bot Behavior Configuration (Optional)
# ========================================

# Custom system prompt (defaults to Discord-friendly formatting guide)
# BOT_SYSTEM_PROMPT=You are a helpful Discord bot assistant.

# Bot personality description (affects tone and style)
# BOT_PERSONALITY=Bubbly, witty, and lightly humorous.

# Maximum messages to keep in conversation memory (default: 10)
BOT_MAX_MEMORY_MESSAGES=10

# Enable automatic summarization of older messages (default: true)
BOT_ENABLE_SUMMARY=true

# Additional trigger names the bot responds to (comma-separated, case-insensitive)
# Example: BOT_TRIGGER_NAMES=bot,assistant,helper
# Leave empty to respond only to mentions and replies
# BOT_TRIGGER_NAMES=

# ========================================
# Advanced: Code Generation & Jobs (Optional)
# ========================================

# Model used for code generation
# Defaults to: google/gemini-3-flash-preview
# CODEGEN_MODEL=google/gemini-3-flash-preview

# Model used for prompt improvement
# Defaults to: openai/gpt-oss-120b
# PROMPT_IMPROVER_MODEL=openai/gpt-oss-120b

# Override model caching capability detection
# Values: 'true' or 'false' (auto-detected if not set)
# MODEL_CACHE_CAPABLE=true

# ========================================
# Advanced: Distributed/Clustering (Optional)
# ========================================

# Unique instance identifier (auto-generated if not set)
# Used for distributed deployments (e.g., Kubernetes, Docker Swarm)
# INSTANCE_ID=instance-1

# Redis connection URL for distributed state (optional)
# Format: redis://user:password@localhost:6379/0
# If not set, bot uses in-memory storage (single instance only)
# REDIS_URL=

# ========================================
# Advanced: Job System Directories (Optional)
# ========================================

# Base directory for job working files
# Defaults to: ./logs/work
# JOB_WORK_BASE=./logs/work

# Base directory for job output files
# Defaults to: ./logs/output
# JOB_OUTPUT_BASE=./logs/output

# Base directory for job logs
# Defaults to: ./logs
# JOB_LOG_BASE=./logs

# ========================================
# Distributed Tier Selection (Optional - Advanced)
# ========================================

# Override tier selection logic
# Values: 'free' or 'paid'
# If not set, tier is selected based on routing logic
# OPENROUTER_TIER=

# ========================================
# GETTING STARTED
# ========================================
# 
# 1. Copy this file:
#    $ cp .env.example .env
#
# 2. Edit .env and fill in required values:
#    - DISCORD_TOKEN
#    - DISCORD_CLIENT_ID
#    - OPENROUTER_API_KEY
#
# 3. (Optional) Review and customize the model configurations and pricing
#
# 4. Run the bot:
#    $ npm run dev              # Development mode with hot reload
#    $ npm run build && npm start  # Production mode
#    $ npm run console          # Console-only mode
#
# ========================================
# SECURITY NOTES
# ========================================
#
# - NEVER commit .env to git
# - .env is listed in .gitignore for safety
# - Treat DISCORD_TOKEN and OPENROUTER_API_KEY as secrets
# - Rotate tokens regularly
# - Use environment-specific secrets in production
#
